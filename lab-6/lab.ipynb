{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow\n",
    "import numpy\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 1.6/33.5 MB 9.3 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 3.1/33.5 MB 8.4 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 4.7/33.5 MB 8.4 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 6.6/33.5 MB 8.2 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 8.1/33.5 MB 8.2 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 9.7/33.5 MB 8.1 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 11.8/33.5 MB 8.2 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 13.6/33.5 MB 8.4 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 15.5/33.5 MB 8.5 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 17.3/33.5 MB 8.5 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 19.1/33.5 MB 8.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 21.2/33.5 MB 8.7 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 23.1/33.5 MB 8.7 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 25.2/33.5 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 27.3/33.5 MB 8.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 29.4/33.5 MB 8.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 31.2/33.5 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  33.0/33.5 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 33.5/33.5 MB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/deeplog.py\n",
    "#!/usr/bin/python3\n",
    "import copy\n",
    "import gc\n",
    "import gzip\n",
    "import json\n",
    "import numpy\n",
    "import os\n",
    "import progressbar\n",
    "import random\n",
    "import spacy\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from models import util, sequence, mimick\n",
    "\n",
    "\n",
    "class Codebook(util.Codebook):\n",
    "    pass\n",
    "\n",
    "\n",
    "class DeeplogSequence(sequence.EventSequence):\n",
    "    \"\"\"\n",
    "        Function: to construct a sequence for Deeplog inherited from EventSequence\n",
    "    \"\"\"\n",
    "    def __init__(self, config=None, sequence=None):\n",
    "        super().__init__(config, sequence)\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")\n",
    "        self.unknowns = 0\n",
    "\n",
    "    def batch(self, mode='deeplog'):\n",
    "        batchsize = self.config.batch\n",
    "        self.config.batch = len(self.sequences)\n",
    "        ret = self.next(mode)\n",
    "        self.config.batch = batchsize\n",
    "        self.reset()\n",
    "        return ret\n",
    "\n",
    "    def next(self, mode='deeplog'):\n",
    "        if not self.ready:\n",
    "            self.reset()\n",
    "        if self.idx >= len(self):\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "        self.idx += 1\n",
    "        return self.__getitem__(self.idx - 1, mode)\n",
    "\n",
    "    def __getitem__(self, idx, mode='deeplog'):\n",
    "        if mode == 'deeplog':\n",
    "            return self.getItemDeeplog(idx)\n",
    "        if mode == 'word2vec_fit':\n",
    "            return self.getItemWord2Vec(idx, epochs=256)\n",
    "        if mode == 'word2vec_predict':\n",
    "            return self.getItemWord2Vec(idx, epochs=1)\n",
    "\n",
    "    def getItemDeeplog(self, idx):\n",
    "        if not self.ready:\n",
    "            self.reset()\n",
    "        XL, XF, YL, INFO = [], [], [], []\n",
    "        begin = idx * self.config.batch\n",
    "        end = (idx + 1) * self.config.batch\n",
    "        maxlen = 0 if not self.config.static_length else self.config.seqlen\n",
    "        for i in range(begin, end):\n",
    "            idx = self.shuffled_indices[i] if self.config.shuffle else i\n",
    "            sequence = [[self.codebook.BEGIN] + [0.0] * self.float_size]\n",
    "            sequence += self.sequences[idx]\n",
    "            sequence += [[self.codebook.END] + [0.0] * self.float_size]\n",
    "            maxlen = max(maxlen, min(self.config.seqlen, len(sequence)))\n",
    "            # build inputs\n",
    "            for j in range(1, len(sequence)):\n",
    "                xl = [self.codebook.enc(event[0]) for event in sequence[max(0, j - self.config.seqlen): j]]\n",
    "                xf = [event[1:] for event in sequence[max(0, j - self.config.seqlen): j]]\n",
    "                yl = [self.codebook.enc(sequence[j][0])]\n",
    "                XL.append(xl)\n",
    "                XF.append(xf)\n",
    "                YL.append(yl)\n",
    "                seqinfo = copy.copy(self.seqinfo[idx])\n",
    "                seqinfo['idx'] = j\n",
    "                INFO.append(seqinfo)\n",
    "        # add paddings and transform\n",
    "        for i in range(0, len(XL)):\n",
    "            if len(XL[i]) < maxlen:\n",
    "                padlen = maxlen - len(XL[i])\n",
    "                XL[i] = [self.codebook.enc(self.codebook.PAD)] * padlen + XL[i]\n",
    "                XF[i] = [[0.0] * self.float_size] * padlen + XF[i]\n",
    "        # finalize\n",
    "        XL = numpy.array(XL)\n",
    "        XF = numpy.array(XF)\n",
    "        for i in range(0, len(YL)):\n",
    "            for j in range(0, len(YL[i])):\n",
    "                if YL[i][j] >= len(self.codebook) - self.unknowns:\n",
    "                    YL[i][j] = self.codebook.enc(self.codebook.UNKNOWN)\n",
    "        YL = keras.utils.to_categorical(YL, num_classes=len(self.codebook) - self.unknowns)\n",
    "        if self.config.verbose:\n",
    "            print('Batch Shape:', XL.shape, XF.shape, YL.shape)\n",
    "        if self.float_size > 0:\n",
    "            return [XL, XF], YL, INFO\n",
    "        return XL, YL, INFO\n",
    "\n",
    "    def getItemWord2Vec(self, idx, epochs=1):\n",
    "        if not self.ready:\n",
    "            self.reset()\n",
    "        # find-and-add unknown events\n",
    "        for event in self.types:\n",
    "            if event not in self.codebook.encode:\n",
    "                self.codebook.add(event)\n",
    "                self.unknowns += 1\n",
    "                print('    \\033[91mAdd Unknown Event to Codebook:\\033[0m', self.codebook.enc(event), event)\n",
    "        # build vectors\n",
    "        vectors, maxlen = [], 0\n",
    "        for event in [self.codebook.decode[i] for i in range(0, len(self.codebook))]:\n",
    "            vectors.append([])\n",
    "            for c in '.:*,_/=':\n",
    "                event = event.replace(c, ' ')\n",
    "            event = self.nlp(event)\n",
    "            maxlen = max(maxlen, len(event))\n",
    "            for word in event:\n",
    "                vectors[-1].append(word.vector)\n",
    "        # add paddings\n",
    "        X, dim = [], len(vectors[0][0])\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in range(0, len(vectors)):\n",
    "                if len(vectors[i]) < maxlen:\n",
    "                    padding, padlen = [], maxlen - len(vectors[i])\n",
    "                    for j in range(0, padlen):\n",
    "                        padding.append([random.random() for _ in range(0, dim)])\n",
    "                    X.append(padding + vectors[i])\n",
    "                else:\n",
    "                    X.append(vectors[i])\n",
    "        # finalize\n",
    "        X = numpy.array(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class Deeplog(object):\n",
    "    @staticmethod\n",
    "    def SetupGPU(gpulist=None):\n",
    "        gpus = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "        if gpulist is None:\n",
    "            lgpus = gpus\n",
    "        elif len(gpulist) == 1 and gpulist[0] == -1:\n",
    "            lgpus = []\n",
    "        else:\n",
    "            lgpus = [gpus[i] for i in gpulist]\n",
    "        tensorflow.config.experimental.set_visible_devices(devices=lgpus, device_type='GPU')\n",
    "        print('\\033[91mUsing GPU\\n', '\\n'.join([str(gpu) for gpu in lgpus]), '\\033[0m')\n",
    "\n",
    "    class Config(object):\n",
    "        def __init__(self, hidden_layer=2, hidden_unit=64, epochs=256, batch_size=2048, rank_threshold=0.05,\n",
    "                     distance_threshold=0.05, use_mimick_embedding=False, filepath=None, use_gzip=True, verbose=False):\n",
    "            self.verbose = verbose\n",
    "            self.hidden_layer = hidden_layer\n",
    "            self.hidden_unit = hidden_unit\n",
    "            self.epochs = epochs\n",
    "            self.batch_size = batch_size\n",
    "            self.rank_threshold = rank_threshold\n",
    "            self.distance_threshold = distance_threshold\n",
    "            self.use_mimick_embedding = use_mimick_embedding\n",
    "            self.filepath = filepath\n",
    "            self.use_gzip = use_gzip\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        self.model = None\n",
    "        self.config = config if config is not None else Deeplog.Config()\n",
    "        self.mimick = None\n",
    "        self.embedding = {}\n",
    "\n",
    "    def pretrain(self, trainset, mode='deeplog'):\n",
    "        if not self.config.use_mimick_embedding:\n",
    "            return\n",
    "        # initialize variables\n",
    "        gc.collect()\n",
    "        n_labels = len(trainset.codebook)\n",
    "        n_floats = trainset.float_size\n",
    "        nn_size = self.config.hidden_unit + n_floats\n",
    "        seqlen = min(trainset.maxlen + 2, trainset.config.seqlen)\n",
    "        # input label sequence\n",
    "        label_input = keras.layers.Input(shape=(None,), name='Label_Input')\n",
    "        label_embed = keras.layers.Embedding(n_labels, self.config.hidden_unit, mask_zero=True, name='Label_Embed')\n",
    "        label_dense = label_embed(label_input)\n",
    "        # input float sequence\n",
    "        global float_input, merge\n",
    "        if n_floats > 0:\n",
    "            float_input = keras.layers.Input(shape=(None, n_floats), name='Float_Input')\n",
    "            float_dense = keras.layers.Dense(n_floats, name='Float_Dense')(float_input)\n",
    "            merge = keras.layers.concatenate([label_dense, float_dense], axis=2)\n",
    "        # neural network layer\n",
    "        nn = keras.models.Sequential(name='RNN')\n",
    "        for i in range(1, self.config.hidden_layer):\n",
    "            nn.add(keras.layers.LSTM(nn_size, activation='relu', return_sequences=True, name='LSTM_' + str(i)))\n",
    "        nn.add(keras.layers.LSTM(nn_size, name='LSTM_' + str(self.config.hidden_layer)))\n",
    "        nn.add(keras.layers.Dense(n_labels, activation='softmax', name='Softmax'))\n",
    "        # build model\n",
    "        if n_floats == 0:\n",
    "            model = keras.models.Model(inputs=label_input, outputs=nn(label_dense))\n",
    "        else:\n",
    "            model = keras.models.Model(inputs=[label_input, float_input], outputs=nn(merge))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "        if self.config.verbose:\n",
    "            model.summary()\n",
    "        if self.config.verbose:\n",
    "            nn.summary()\n",
    "        # train deeplog\n",
    "        X, Y, I = trainset.batch(mode=mode)\n",
    "        callbacks = [keras.callbacks.EarlyStopping(monitor='loss', patience=64)]\n",
    "        model.fit(X, Y, shuffle=True, verbose=self.config.verbose, batch_size=self.config.batch_size,\n",
    "                  epochs=self.config.epochs, callbacks=callbacks)\n",
    "        # get embedding\n",
    "        embedding = list(label_embed.get_weights()[0])\n",
    "        # destroy network\n",
    "        keras.backend.clear_session()\n",
    "        del model\n",
    "        del X\n",
    "        del Y\n",
    "        del I\n",
    "        gc.collect()\n",
    "        # build and train mimick\n",
    "        X, Y = trainset.batch(mode='word2vec_fit'), []\n",
    "        for i in range(0, len(X) // len(embedding)):\n",
    "            Y.extend(embedding)\n",
    "        Y = numpy.array(Y)\n",
    "        self.mimick = mimick.Word2VecMimickEmbedding(X.shape[2], Y.shape[1], filepath=self.config.filepath,\n",
    "                                                     verbose=self.config.verbose)\n",
    "        self.mimick.train(X, Y)\n",
    "\n",
    "    def train(self, trainset):\n",
    "        return self.fit(trainset)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        if self.model is None:\n",
    "            # data preprocess\n",
    "            gc.collect()\n",
    "            self.n_labels = len(trainset.codebook)\n",
    "            self.n_floats = trainset.float_size\n",
    "            self.nn_size = self.config.hidden_unit + self.n_floats\n",
    "            # input label sequence\n",
    "            if self.mimick is None:\n",
    "                self.label_input = keras.layers.Input(shape=(None,), name='Label_Input')\n",
    "                self.label_embed = keras.layers.Embedding(self.n_labels, self.config.hidden_unit, mask_zero=True,\n",
    "                                                          name='Label_Embed')\n",
    "                self.label_dense = self.label_embed(self.label_input)\n",
    "            else:\n",
    "                self.label_input = keras.layers.Input(shape=(None, self.config.hidden_unit), name='Label_Input')\n",
    "                self.label_dense = keras.layers.Dense(self.config.hidden_unit)(self.label_input)\n",
    "                # self.label_dense = keras.layers.Dense (self.config.hidden_unit) (self.label_dense)\n",
    "            # input float sequence\n",
    "            if self.n_floats > 0:\n",
    "                self.float_input = keras.layers.Input(shape=(None, self.n_floats), name='Float_Input')\n",
    "                self.float_dense = keras.layers.Dense(self.n_floats, name='Float_Dense')(self.float_input)\n",
    "                if self.mimick is None:\n",
    "                    self.merge = keras.layers.concatenate([self.label_dense, self.float_dense], axis=2)\n",
    "                else:\n",
    "                    self.merge = keras.layers.concatenate([self.label_input, self.float_dense], axis=2)\n",
    "            # neural network layer\n",
    "            self.nn = keras.models.Sequential(name='RNN')\n",
    "            for i in range(1, self.config.hidden_layer):\n",
    "                self.nn.add(keras.layers.LSTM(self.nn_size, activation='relu', return_sequences=True, name='LSTM_' + str(i)))\n",
    "\n",
    "            self.nn.add(keras.layers.LSTM(self.nn_size, name='LSTM_' + str(self.config.hidden_layer)))\n",
    "            self.nn.add(keras.layers.Dense(self.n_labels, activation='softmax', name='Softmax'))\n",
    "            # model\n",
    "            if self.n_floats == 0:\n",
    "                if self.mimick is None:\n",
    "                    self.model = keras.models.Model(inputs=self.label_input, outputs=self.nn(self.label_dense))\n",
    "                else:\n",
    "                    self.model = keras.models.Model(inputs=self.label_input, outputs=self.nn(self.label_input))\n",
    "            else:\n",
    "                self.model = keras.models.Model(inputs=[self.label_input, self.float_input], outputs=self.nn(self.merge))\n",
    "            self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "            if self.config.verbose:\n",
    "                self.model.summary()\n",
    "            if self.config.verbose:\n",
    "                self.nn.summary()\n",
    "        X, Y, I, V = self.getBatch(trainset)\n",
    "        del X\n",
    "        gc.collect()\n",
    "        return self.model.fit(V, Y, shuffle=True, verbose=self.config.verbose, batch_size=self.config.batch_size,\n",
    "                              epochs=self.config.epochs, callbacks=[keras.callbacks.EarlyStopping(monitor='loss', patience=64)])\n",
    "\n",
    "    def getBatch(self, dataset):\n",
    "        if self.mimick is not None:\n",
    "            # update embedding\n",
    "            self.embedding = self.mimick.predict(dataset.batch(mode='word2vec_predict'))\n",
    "            # extract batch\n",
    "            X, Y, I = dataset.batch()\n",
    "            XL = X[0] if self.n_floats > 0 else X\n",
    "            shape = XL.shape\n",
    "            XL = XL.tolist()\n",
    "            # replace batch with vectors\n",
    "            for seq in range(0, shape[0]):\n",
    "                for evt in range(0, shape[1]):\n",
    "                    XL[seq][evt] = self.embedding[XL[seq][evt]]\n",
    "            V = [numpy.array(XL), X[1]] if self.n_floats > 0 else numpy.array(XL)\n",
    "            print('Batch Shape: [', shape, '--->', V.shape, ']', X[1].shape, Y.shape)\n",
    "        else:\n",
    "            X, Y, I = dataset.batch()\n",
    "            V = X\n",
    "        return X, Y, I, V\n",
    "\n",
    "    def test(self, testset):\n",
    "        return self.predict(testset)\n",
    "\n",
    "    def predict(self, testset):\n",
    "        # temporarily overwrite / save-and-restore config for getBatch\n",
    "        config = testset.config.static_length\n",
    "        testset.config.static_length = True\n",
    "        X, Y, I, V = self.getBatch(testset)\n",
    "        testset.config.static_length = config\n",
    "        # test\n",
    "        if self.config.verbose:\n",
    "            print('Testing')\n",
    "        P = self.model.predict(V, batch_size=self.config.batch_size, verbose=False)\n",
    "        del V\n",
    "        #gc.collect()\n",
    "        if self.config.verbose:\n",
    "            print('Done Testing')\n",
    "        # check the misses and return anomalies\n",
    "        #dij = util.EmbeddingDistance(self.getEmbeddings())\n",
    "        filepath = os.path.join(self.config.filepath, 'predict')\n",
    "        fout = gzip.open(filepath + '.gz', 'wt') if self.config.use_gzip else open(filepath, 'w')\n",
    "        if self.config.use_gzip:\n",
    "            filepath = filepath + '.gz'\n",
    "        n_miss = 0\n",
    "\n",
    "        global bar\n",
    "        if self.config.verbose:\n",
    "            print('Deriving Anomalies')\n",
    "            bar = progressbar.ProgressBar(term_width=100, maxval=len(Y), widgets=[\n",
    "                progressbar.Bar('=', '[', ']'), ' ',\n",
    "                progressbar.Percentage(), ' ',\n",
    "                progressbar.Timer(), ' ',\n",
    "                progressbar.ETA()])\n",
    "            bar.start()\n",
    "\n",
    "        for i in range(0, len(Y)):\n",
    "            if self.config.verbose and i & 1023 == 0:\n",
    "                bar.update(i)\n",
    "            # first condition: the label is not top-predicted\n",
    "            # get the rank of label by finding the corresponding prob in the sorted probs\n",
    "            label = numpy.argmax(Y[i])\n",
    "            sortProbs = sorted(P[i], reverse=True)\n",
    "            rank = float(sortProbs.index(P[i][label])) / len(Y[i])\n",
    "            # conditions\n",
    "            append_to_result = False\n",
    "            if rank > self.config.rank_threshold:\n",
    "                append_to_result = True\n",
    "            # else:  # second condition: the label is far away from top-predicts\n",
    "            #     prob_threshold = sortProbs[min(int(self.config.rank_threshold * len(Y[i])) + 1, len(Y[i]) - 1)]\n",
    "            #     dists = [dij[label][j] if P[i][j] > prob_threshold else 1.0 for j in range(0, len(P[i]))]\n",
    "            #     if len(dists) > 0 and min(dists) > self.config.distance_threshold: append_to_result = True\n",
    "            if append_to_result:  # append prediction-miss\n",
    "                fout.write(\n",
    "                    json.dumps({'X': X[i].tolist() + [int(numpy.argmax(Y[i]))], 'P': P[i].tolist(), 'I': I[i]}) + '\\n')\n",
    "                n_miss += 1\n",
    "        if self.config.verbose:\n",
    "            bar.finish()\n",
    "        return n_miss, self.readfile(filepath)\n",
    "\n",
    "    def getEmbeddings(self, codebook=None):\n",
    "        if len(self.embedding) == 0:\n",
    "            self.embedding = self.label_embed.get_weights()[0]\n",
    "        if codebook is not None:\n",
    "            return {codebook.dec(i): self.embedding[i] for i in range(0, len(codebook))}\n",
    "        return {i: self.embedding[i] for i in range(0, len(self.embedding))}\n",
    "\n",
    "    @staticmethod\n",
    "    def split(miss):\n",
    "        ret, X, P, I = [], miss['X'], miss['P'], miss['I']\n",
    "        idx = len(X) - 1\n",
    "        x, y = X[idx], numpy.argmax(P)\n",
    "        if x != y:\n",
    "            ret.append({'X': X, 'P': P, 'I': I, 'idx': idx, 'x': x, 'y': y})\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def readfile(filepath):\n",
    "        if filepath.endswith('.gz'):\n",
    "            fin = gzip.open(filepath, r)\n",
    "        else:\n",
    "            fin = open(filepath, 'r')\n",
    "        \n",
    "        for line in fin:\n",
    "            yield json.loads(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
